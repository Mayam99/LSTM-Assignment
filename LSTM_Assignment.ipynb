{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Reading the file for the processing the data"
      ],
      "metadata": {
        "id": "2lDmAWobEGYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The file is uploaded as 'LSTM Data.txt'\n",
        "with open('LSTM_DATA.txt', encoding='utf-8') as file:\n",
        "    data = file.read()\n",
        "\n",
        "# Print the first 500 characters of the data\n",
        "print(data[:500])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xtXzffs5bQ7",
        "outputId": "74317c4a-fb77-4eaf-93cd-fe50b181ffd4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Project Gutenberg eBook of Pride and Prejudice\n",
            "    \n",
            "This ebook is for the use of anyone anywhere in the United States and\n",
            "most other parts of the world at no cost and with almost no restrictions\n",
            "whatsoever. You may copy it, give it away or re-use it under the terms\n",
            "of the Project Gutenberg License included with this ebook or online\n",
            "at www.gutenberg.org. If you are not located in the United States,\n",
            "you will have to check the laws of the country where you are located\n",
            "before using this eBook.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importing Necessary Libraries"
      ],
      "metadata": {
        "id": "nHVhi3wm_pgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Embedding,Dense,LSTM, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "yZIsl4Nj8Uuq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Preprocessing the textual data by tokenizing and generating sequences"
      ],
      "metadata": {
        "id": "CN4TQTd-AsVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The 'data' is the input text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "seqence_data = tokenizer.texts_to_sequences([data])[0]\n",
        "\n",
        "# Saving the tokenizer\n",
        "with open('token.pkl', 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "\n",
        "# Printing the first 15 elements of the sequence data\n",
        "print(seqence_data[:15])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08_CWmyl8dom",
        "outputId": "c4898081-c736-4ac3-bd25-7c760d408299"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 187, 149, 98, 2, 60, 3, 72, 29, 98, 8, 16, 1, 150, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###len(seqence_data) tells us how many items are in the list called seqence_data.\n"
      ],
      "metadata": {
        "id": "yFvzEZ93BoaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(seqence_data) # seqence_data is a list of words or tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ep0JSEGb-K6L",
        "outputId": "eb9423ba-ce95-4604-cc7e-06e9d4dbc401"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4598"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###tokenizer.word_index is a part of the Tokenizer object that contains a dictionary. In this dictionary, each unique word from your text data is paired with a number. This number represents the word's index or position in the tokenizer's vocabulary."
      ],
      "metadata": {
        "id": "_PkhXa-gDeqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(tokenizer.word_index) #len(tokenizer.word_index) counts how many unique words (or tokens) are in the text data.\n",
        "vocab_size #vocab_size is just a number that tells you how many different words are in your text data."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APtZvAh1CSph",
        "outputId": "da36d806-59bf-47cd-b5d8-293b78c64fd0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1447"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###This code takes a list of words (seqence_data) and creates sequences of four consecutive words. It then prints the number of sequences created and the first 10 sequences. NumPy is used to handle the data efficiently."
      ],
      "metadata": {
        "id": "cb6Sk9sUHfPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # Numerical computation\n",
        "\n",
        "sequence = [] # initializes an empty list named sequence. This list will be used to store sequences of words.\n",
        "for i in range(3, len(seqence_data)): # iterates through the seqence_data list starting from the index 3. seqence_data contains sequences of words, and this loop is used to create new sequences with four words each.\n",
        "    words = seqence_data[i-3:i+1] # This creates a sliding window of four consecutive words.\n",
        "    sequence.append(words) # adds each sequence of four words to the sequence list.\n",
        "\n",
        "print('The Length of sequence is:', len(sequence)) # prints the length of the sequence list.\n",
        "sequence = np.array(sequence) # converts the list of sequences into a NumPy array\n",
        "print(sequence[:10]) #  Prints the first 10 sequences from the sequence array."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJ-ILhCkCV9w",
        "outputId": "a811a819-1737-434c-dfa1-b7e325169ec8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Length of sequence is: 4595\n",
            "[[  1 187 149  98]\n",
            " [187 149  98   2]\n",
            " [149  98   2  60]\n",
            " [ 98   2  60   3]\n",
            " [  2  60   3  72]\n",
            " [ 60   3  72  29]\n",
            " [  3  72  29  98]\n",
            " [ 72  29  98   8]\n",
            " [ 29  98   8  16]\n",
            " [ 98   8  16   1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = [] # creates an empty list named x.\n",
        "y = [] # creates another empty list named y.\n",
        "for i in sequence: # iterates through each sequence in the sequence list\n",
        "    x.append(i[0:3]) # takes the first three elements of the current sequence i and adds them to the x list. This creates a new list of sequences, each containing the first three words.\n",
        "    y.append(i[3]) # takes the fourth element of the current sequence i and adds it to the y list. This creates a new list of single words, each corresponding to the next word after the first three in each sequence.\n",
        "\n",
        "x = np.array(x) # converts the list of sequences x into a NumPy array.\n",
        "y = np.array(y) # converts the list of single words y into a NumPy array."
      ],
      "metadata": {
        "id": "C7IH2xaJG-lW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x.shape) # prints the shape of the NumPy array x\n",
        "print(y.shape) # prints the shape of the NumPy array y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVl-_1lwM2RJ",
        "outputId": "1b0cae1e-cbc9-467a-b426-9f4dbf027954"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4595, 3)\n",
            "(4595,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Convert the numerical labels in array y into one-hot encoded vectors."
      ],
      "metadata": {
        "id": "nHslFxVwO_Ec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = to_categorical(y,num_classes=7560)"
      ],
      "metadata": {
        "id": "f4dC3FmCNKzM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Splitting the data:"
      ],
      "metadata": {
        "id": "eYIRB7eVQn5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "xtrain,xtest,ytrain,ytest = train_test_split(x,y,test_size=.1)"
      ],
      "metadata": {
        "id": "yi8-xtO5NnLP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Building the model"
      ],
      "metadata": {
        "id": "oLUggXwYRUT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(7560, 10,  input_length=3))\n",
        "model.add(LSTM(1000,return_sequences=True))\n",
        "model.add(LSTM(1000))\n",
        "model.add(Dense(1000, activation='relu'))\n",
        "model.add(Dense(7560, activation='softmax'))"
      ],
      "metadata": {
        "id": "2eqE9Yu3Q508"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oo8rfUO6RO6v",
        "outputId": "f48a2914-f775-4734-8b94-1eea43927228"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 3, 10)             75600     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 3, 1000)           4044000   \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 1000)              8004000   \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1000)              1001000   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 7560)              7567560   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20692160 (78.93 MB)\n",
            "Trainable params: 20692160 (78.93 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',optimizer=Adam(learning_rate=0.001),metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "fgS5pKI9Rhxg"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(xtrain,ytrain,validation_data=(xtest,ytest),epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YneG1yh0Rw2D",
        "outputId": "7877c976-dd7a-40db-fc54-eed0d1d6f8c3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "130/130 [==============================] - 14s 43ms/step - loss: 7.1824 - accuracy: 0.0600 - val_loss: 6.7920 - val_accuracy: 0.0478\n",
            "Epoch 2/100\n",
            "130/130 [==============================] - 3s 23ms/step - loss: 6.2106 - accuracy: 0.0638 - val_loss: 7.0278 - val_accuracy: 0.0478\n",
            "Epoch 3/100\n",
            "130/130 [==============================] - 3s 20ms/step - loss: 6.0859 - accuracy: 0.0612 - val_loss: 7.2684 - val_accuracy: 0.0478\n",
            "Epoch 4/100\n",
            "130/130 [==============================] - 2s 19ms/step - loss: 6.0262 - accuracy: 0.0638 - val_loss: 7.5323 - val_accuracy: 0.0478\n",
            "Epoch 5/100\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 5.8997 - accuracy: 0.0631 - val_loss: 7.4225 - val_accuracy: 0.0522\n",
            "Epoch 6/100\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 5.7674 - accuracy: 0.0718 - val_loss: 7.9363 - val_accuracy: 0.0717\n",
            "Epoch 7/100\n",
            "130/130 [==============================] - 2s 18ms/step - loss: 5.6065 - accuracy: 0.0808 - val_loss: 7.6963 - val_accuracy: 0.0630\n",
            "Epoch 8/100\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 5.3615 - accuracy: 0.0871 - val_loss: 7.6941 - val_accuracy: 0.0761\n",
            "Epoch 9/100\n",
            "130/130 [==============================] - 2s 18ms/step - loss: 5.0923 - accuracy: 0.0955 - val_loss: 7.9855 - val_accuracy: 0.0761\n",
            "Epoch 10/100\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 4.8614 - accuracy: 0.0958 - val_loss: 8.5667 - val_accuracy: 0.0543\n",
            "Epoch 11/100\n",
            "130/130 [==============================] - 2s 16ms/step - loss: 4.6101 - accuracy: 0.1042 - val_loss: 8.8781 - val_accuracy: 0.0565\n",
            "Epoch 12/100\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 4.3315 - accuracy: 0.1151 - val_loss: 9.4072 - val_accuracy: 0.0326\n",
            "Epoch 13/100\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 4.0847 - accuracy: 0.1296 - val_loss: 10.0599 - val_accuracy: 0.0413\n",
            "Epoch 14/100\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 3.7904 - accuracy: 0.1352 - val_loss: 11.2299 - val_accuracy: 0.0478\n",
            "Epoch 15/100\n",
            "130/130 [==============================] - 2s 16ms/step - loss: 3.5223 - accuracy: 0.1560 - val_loss: 11.8192 - val_accuracy: 0.0370\n",
            "Epoch 16/100\n",
            "130/130 [==============================] - 2s 16ms/step - loss: 3.3017 - accuracy: 0.1845 - val_loss: 12.4824 - val_accuracy: 0.0413\n",
            "Epoch 17/100\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 3.0794 - accuracy: 0.2060 - val_loss: 12.9419 - val_accuracy: 0.0348\n",
            "Epoch 18/100\n",
            "130/130 [==============================] - 2s 18ms/step - loss: 2.8663 - accuracy: 0.2426 - val_loss: 13.1566 - val_accuracy: 0.0348\n",
            "Epoch 19/100\n",
            "130/130 [==============================] - 2s 18ms/step - loss: 2.6861 - accuracy: 0.2684 - val_loss: 14.1980 - val_accuracy: 0.0261\n",
            "Epoch 20/100\n",
            "130/130 [==============================] - 2s 16ms/step - loss: 2.4808 - accuracy: 0.3156 - val_loss: 15.2140 - val_accuracy: 0.0370\n",
            "Epoch 21/100\n",
            "130/130 [==============================] - 2s 16ms/step - loss: 2.3273 - accuracy: 0.3545 - val_loss: 15.8650 - val_accuracy: 0.0326\n",
            "Epoch 22/100\n",
            "130/130 [==============================] - 2s 18ms/step - loss: 2.1492 - accuracy: 0.3872 - val_loss: 16.5965 - val_accuracy: 0.0326\n",
            "Epoch 23/100\n",
            "130/130 [==============================] - 2s 18ms/step - loss: 2.0051 - accuracy: 0.4239 - val_loss: 17.2215 - val_accuracy: 0.0326\n",
            "Epoch 24/100\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 1.9061 - accuracy: 0.4469 - val_loss: 18.0209 - val_accuracy: 0.0391\n",
            "Epoch 25/100\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 1.7203 - accuracy: 0.4907 - val_loss: 19.0487 - val_accuracy: 0.0413\n",
            "Epoch 26/100\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 1.5787 - accuracy: 0.5260 - val_loss: 20.1031 - val_accuracy: 0.0478\n",
            "Epoch 27/100\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 1.5053 - accuracy: 0.5410 - val_loss: 20.0996 - val_accuracy: 0.0370\n",
            "Epoch 28/100\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 1.2906 - accuracy: 0.6012 - val_loss: 21.3464 - val_accuracy: 0.0348\n",
            "Epoch 29/100\n",
            "130/130 [==============================] - 3s 20ms/step - loss: 1.2177 - accuracy: 0.6297 - val_loss: 22.6265 - val_accuracy: 0.0391\n",
            "Epoch 30/100\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 1.0753 - accuracy: 0.6742 - val_loss: 22.3045 - val_accuracy: 0.0457\n",
            "Epoch 31/100\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 0.9551 - accuracy: 0.7105 - val_loss: 24.1499 - val_accuracy: 0.0435\n",
            "Epoch 32/100\n",
            "130/130 [==============================] - 2s 16ms/step - loss: 0.9080 - accuracy: 0.7151 - val_loss: 24.2060 - val_accuracy: 0.0413\n",
            "Epoch 33/100\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 0.8138 - accuracy: 0.7495 - val_loss: 25.1560 - val_accuracy: 0.0457\n",
            "Epoch 34/100\n",
            "130/130 [==============================] - 2s 18ms/step - loss: 0.7200 - accuracy: 0.7785 - val_loss: 26.3228 - val_accuracy: 0.0370\n",
            "Epoch 35/100\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 0.6726 - accuracy: 0.7971 - val_loss: 27.0954 - val_accuracy: 0.0543\n",
            "Epoch 36/100\n",
            "130/130 [==============================] - 2s 16ms/step - loss: 0.6648 - accuracy: 0.7971 - val_loss: 26.5890 - val_accuracy: 0.0348\n",
            "Epoch 37/100\n",
            "130/130 [==============================] - 2s 16ms/step - loss: 0.5783 - accuracy: 0.8244 - val_loss: 27.1898 - val_accuracy: 0.0391\n",
            "Epoch 38/100\n",
            "130/130 [==============================] - 2s 16ms/step - loss: 0.4954 - accuracy: 0.8597 - val_loss: 27.4301 - val_accuracy: 0.0391\n",
            "Epoch 39/100\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 0.4316 - accuracy: 0.8776 - val_loss: 28.2976 - val_accuracy: 0.0435\n",
            "Epoch 40/100\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 0.3754 - accuracy: 0.8900 - val_loss: 27.9983 - val_accuracy: 0.0457\n",
            "Epoch 41/100\n",
            "130/130 [==============================] - 2s 16ms/step - loss: 0.3380 - accuracy: 0.9025 - val_loss: 28.7373 - val_accuracy: 0.0478\n",
            "Epoch 42/100\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 0.3095 - accuracy: 0.9141 - val_loss: 28.6749 - val_accuracy: 0.0543\n",
            "Epoch 43/100\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 0.2791 - accuracy: 0.9209 - val_loss: 29.1271 - val_accuracy: 0.0522\n",
            "Epoch 44/100\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 0.2911 - accuracy: 0.9192 - val_loss: 29.5704 - val_accuracy: 0.0457\n",
            "Epoch 45/100\n",
            "130/130 [==============================] - 2s 18ms/step - loss: 0.2926 - accuracy: 0.9180 - val_loss: 29.9212 - val_accuracy: 0.0500\n",
            "Epoch 46/100\n",
            "130/130 [==============================] - 2s 16ms/step - loss: 0.3117 - accuracy: 0.9093 - val_loss: 29.4369 - val_accuracy: 0.0478\n",
            "Epoch 47/100\n",
            "130/130 [==============================] - 2s 16ms/step - loss: 0.3679 - accuracy: 0.8938 - val_loss: 28.4768 - val_accuracy: 0.0522\n",
            "Epoch 48/100\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 0.3837 - accuracy: 0.8885 - val_loss: 28.1458 - val_accuracy: 0.0478\n",
            "Epoch 49/100\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 0.3626 - accuracy: 0.8936 - val_loss: 28.4044 - val_accuracy: 0.0543\n",
            "Epoch 50/100\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 0.2579 - accuracy: 0.9253 - val_loss: 29.0109 - val_accuracy: 0.0565\n",
            "Epoch 51/100\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 0.2068 - accuracy: 0.9424 - val_loss: 29.0196 - val_accuracy: 0.0522\n",
            "Epoch 52/100\n",
            "130/130 [==============================] - 2s 16ms/step - loss: 0.1807 - accuracy: 0.9480 - val_loss: 28.8019 - val_accuracy: 0.0587\n",
            "Epoch 53/100\n",
            "130/130 [==============================] - 2s 16ms/step - loss: 0.1868 - accuracy: 0.9466 - val_loss: 29.0172 - val_accuracy: 0.0543\n",
            "Epoch 54/100\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 0.1831 - accuracy: 0.9470 - val_loss: 28.6411 - val_accuracy: 0.0500\n",
            "Epoch 55/100\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 0.1676 - accuracy: 0.9473 - val_loss: 29.4645 - val_accuracy: 0.0609\n",
            "Epoch 56/100\n",
            "130/130 [==============================] - 2s 19ms/step - loss: 0.1645 - accuracy: 0.9499 - val_loss: 29.4116 - val_accuracy: 0.0543\n",
            "Epoch 57/100\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 0.1948 - accuracy: 0.9441 - val_loss: 29.4489 - val_accuracy: 0.0522\n",
            "Epoch 58/100\n",
            "130/130 [==============================] - 2s 16ms/step - loss: 0.2434 - accuracy: 0.9284 - val_loss: 29.1762 - val_accuracy: 0.0522\n",
            "Epoch 59/100\n",
            "130/130 [==============================] - 2s 16ms/step - loss: 0.2745 - accuracy: 0.9129 - val_loss: 27.7938 - val_accuracy: 0.0565\n",
            "Epoch 60/100\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 0.2725 - accuracy: 0.9137 - val_loss: 28.2220 - val_accuracy: 0.0587\n",
            "Epoch 61/100\n",
            "130/130 [==============================] - 2s 18ms/step - loss: 0.3601 - accuracy: 0.8888 - val_loss: 27.6648 - val_accuracy: 0.0413\n",
            "Epoch 62/100\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 0.3020 - accuracy: 0.9134 - val_loss: 27.5488 - val_accuracy: 0.0435\n",
            "Epoch 63/100\n",
            "130/130 [==============================] - 2s 16ms/step - loss: 0.2018 - accuracy: 0.9345 - val_loss: 28.0922 - val_accuracy: 0.0413\n",
            "Epoch 64/100\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 0.1680 - accuracy: 0.9473 - val_loss: 28.0919 - val_accuracy: 0.0457\n",
            "Epoch 65/100\n",
            "130/130 [==============================] - 2s 16ms/step - loss: 0.1340 - accuracy: 0.9557 - val_loss: 27.7095 - val_accuracy: 0.0543\n",
            "Epoch 66/100\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 0.1312 - accuracy: 0.9526 - val_loss: 27.9143 - val_accuracy: 0.0435\n",
            "Epoch 67/100\n",
            "130/130 [==============================] - 2s 18ms/step - loss: 0.1655 - accuracy: 0.9478 - val_loss: 28.4519 - val_accuracy: 0.0500\n",
            "Epoch 68/100\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 0.1457 - accuracy: 0.9548 - val_loss: 28.0866 - val_accuracy: 0.0413\n",
            "Epoch 69/100\n",
            "130/130 [==============================] - 2s 16ms/step - loss: 0.1185 - accuracy: 0.9594 - val_loss: 27.7081 - val_accuracy: 0.0478\n",
            "Epoch 70/100\n",
            "130/130 [==============================] - 2s 18ms/step - loss: 0.1095 - accuracy: 0.9603 - val_loss: 27.3497 - val_accuracy: 0.0478\n",
            "Epoch 71/100\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 0.1117 - accuracy: 0.9611 - val_loss: 27.6419 - val_accuracy: 0.0500\n",
            "Epoch 72/100\n",
            "130/130 [==============================] - 2s 18ms/step - loss: 0.1072 - accuracy: 0.9623 - val_loss: 28.0128 - val_accuracy: 0.0435\n",
            "Epoch 73/100\n",
            "130/130 [==============================] - 2s 16ms/step - loss: 0.0948 - accuracy: 0.9654 - val_loss: 28.0832 - val_accuracy: 0.0478\n",
            "Epoch 74/100\n",
            "130/130 [==============================] - 2s 16ms/step - loss: 0.1032 - accuracy: 0.9664 - val_loss: 27.7905 - val_accuracy: 0.0435\n",
            "Epoch 75/100\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 0.0890 - accuracy: 0.9674 - val_loss: 27.6614 - val_accuracy: 0.0522\n",
            "Epoch 76/100\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 0.1199 - accuracy: 0.9594 - val_loss: 27.7930 - val_accuracy: 0.0457\n",
            "Epoch 77/100\n",
            "130/130 [==============================] - 2s 18ms/step - loss: 0.3873 - accuracy: 0.8856 - val_loss: 26.8874 - val_accuracy: 0.0500\n",
            "Epoch 78/100\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 0.4923 - accuracy: 0.8462 - val_loss: 25.7810 - val_accuracy: 0.0457\n",
            "Epoch 79/100\n",
            "130/130 [==============================] - 2s 16ms/step - loss: 0.3151 - accuracy: 0.8975 - val_loss: 27.1253 - val_accuracy: 0.0500\n",
            "Epoch 80/100\n",
            "130/130 [==============================] - 2s 16ms/step - loss: 0.1819 - accuracy: 0.9398 - val_loss: 27.9366 - val_accuracy: 0.0522\n",
            "Epoch 81/100\n",
            "130/130 [==============================] - 2s 16ms/step - loss: 0.1231 - accuracy: 0.9572 - val_loss: 26.9435 - val_accuracy: 0.0543\n",
            "Epoch 82/100\n",
            "130/130 [==============================] - 2s 16ms/step - loss: 0.0980 - accuracy: 0.9632 - val_loss: 27.3184 - val_accuracy: 0.0522\n",
            "Epoch 83/100\n",
            "130/130 [==============================] - 2s 18ms/step - loss: 0.0875 - accuracy: 0.9640 - val_loss: 27.0576 - val_accuracy: 0.0543\n",
            "Epoch 84/100\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 0.0691 - accuracy: 0.9695 - val_loss: 27.0519 - val_accuracy: 0.0522\n",
            "Epoch 85/100\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 0.0641 - accuracy: 0.9681 - val_loss: 26.8460 - val_accuracy: 0.0500\n",
            "Epoch 86/100\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 0.0660 - accuracy: 0.9712 - val_loss: 27.0067 - val_accuracy: 0.0543\n",
            "Epoch 87/100\n",
            "130/130 [==============================] - 2s 16ms/step - loss: 0.0678 - accuracy: 0.9690 - val_loss: 26.6486 - val_accuracy: 0.0522\n",
            "Epoch 88/100\n",
            "130/130 [==============================] - 2s 18ms/step - loss: 0.0620 - accuracy: 0.9722 - val_loss: 26.5325 - val_accuracy: 0.0522\n",
            "Epoch 89/100\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 0.0608 - accuracy: 0.9705 - val_loss: 26.9782 - val_accuracy: 0.0522\n",
            "Epoch 90/100\n",
            "130/130 [==============================] - 2s 16ms/step - loss: 0.0629 - accuracy: 0.9688 - val_loss: 26.6200 - val_accuracy: 0.0500\n",
            "Epoch 91/100\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 0.0598 - accuracy: 0.9705 - val_loss: 26.6915 - val_accuracy: 0.0565\n",
            "Epoch 92/100\n",
            "130/130 [==============================] - 2s 18ms/step - loss: 0.0605 - accuracy: 0.9722 - val_loss: 26.6887 - val_accuracy: 0.0543\n",
            "Epoch 93/100\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 0.0604 - accuracy: 0.9707 - val_loss: 26.6223 - val_accuracy: 0.0543\n",
            "Epoch 94/100\n",
            "130/130 [==============================] - 2s 18ms/step - loss: 0.0600 - accuracy: 0.9705 - val_loss: 26.6756 - val_accuracy: 0.0522\n",
            "Epoch 95/100\n",
            "130/130 [==============================] - 2s 16ms/step - loss: 0.0616 - accuracy: 0.9698 - val_loss: 26.6268 - val_accuracy: 0.0522\n",
            "Epoch 96/100\n",
            "130/130 [==============================] - 2s 16ms/step - loss: 0.0625 - accuracy: 0.9703 - val_loss: 26.2040 - val_accuracy: 0.0543\n",
            "Epoch 97/100\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 0.0624 - accuracy: 0.9695 - val_loss: 26.3952 - val_accuracy: 0.0478\n",
            "Epoch 98/100\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 0.1177 - accuracy: 0.9557 - val_loss: 27.3359 - val_accuracy: 0.0587\n",
            "Epoch 99/100\n",
            "130/130 [==============================] - 2s 18ms/step - loss: 0.7228 - accuracy: 0.7985 - val_loss: 24.2936 - val_accuracy: 0.0435\n",
            "Epoch 100/100\n",
            "130/130 [==============================] - 2s 16ms/step - loss: 0.5497 - accuracy: 0.8336 - val_loss: 23.8245 - val_accuracy: 0.0500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Predict Next Words Model\n"
      ],
      "metadata": {
        "id": "y5d_0SkdZKWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import time\n",
        "text = \"To some the delightful freshness and humour\"\n",
        "\n",
        "for i in range(15):\n",
        "  # tokenize\n",
        "  token_text = tokenizer.texts_to_sequences([text])[0]\n",
        "  # padding\n",
        "  padded_token_text = pad_sequences([token_text], maxlen=3, padding='pre')\n",
        "  # predict\n",
        "  pos = np.argmax(model.predict(padded_token_text))\n",
        "\n",
        "  for word,index in tokenizer.word_index.items():\n",
        "    if index == pos:\n",
        "      text = text + \" \" + word\n",
        "      print(text)\n",
        "      time.sleep(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntdnnaIbXRG6",
        "outputId": "02cd21cd-7892-4c48-d53c-f51c31f92e1d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 20ms/step\n",
            "To some the delightful freshness and humour of\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "To some the delightful freshness and humour of northanger\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "To some the delightful freshness and humour of northanger abbey\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "To some the delightful freshness and humour of northanger abbey its\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "To some the delightful freshness and humour of northanger abbey its completeness\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "To some the delightful freshness and humour of northanger abbey its completeness finish\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "To some the delightful freshness and humour of northanger abbey its completeness finish and\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "To some the delightful freshness and humour of northanger abbey its completeness finish and another\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "To some the delightful freshness and humour of northanger abbey its completeness finish and another grant\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "To some the delightful freshness and humour of northanger abbey its completeness finish and another grant not\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "To some the delightful freshness and humour of northanger abbey its completeness finish and another grant not at\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "To some the delightful freshness and humour of northanger abbey its completeness finish and another grant not at the\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "To some the delightful freshness and humour of northanger abbey its completeness finish and another grant not at the early\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "To some the delightful freshness and humour of northanger abbey its completeness finish and another grant not at the early mechanic\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "To some the delightful freshness and humour of northanger abbey its completeness finish and another grant not at the early mechanic back\n"
          ]
        }
      ]
    }
  ]
}